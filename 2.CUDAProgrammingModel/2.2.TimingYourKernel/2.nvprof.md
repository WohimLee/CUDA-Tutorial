
# nvprof

## 1 nvprof
CUDA 5.0 之后的版本，NVIDIA 提供了 nvprof 命令行分析工具，可以帮助从应用程序的 CPU 和 GPU 活动情况中获取时间线信息，其包括 `内核执行`、`内存传输` 以及 `CUDA API 的调用`

nvprof 可以让你知道程序所花费的时间主要用在哪里

>nvprof
```c++
nvprof [nvprof_args] [Application] [application_args] 
```

>帮助文档
```c++
nvprof --help
```


>计算时间与传输时间
- 如果程序用于计算的时间大于数据传输所用的时间，那么或许可以压缩这些操作，并完全隐藏与传输数据有关的延迟
- 如果程序用于计算的时间少于数据传输所用的时间，那么需要尽量减少 Host 和 Device 之间的传输。后面可以用 `Stream` 和 `Event` 来压缩 `计算量` 和 `通信量`

&emsp;
## 2 性能优化分析
在进行程序优化时，如何将程序和理论界限进行比较是很重要的。


由 nvprof 得到的计数器可以帮助你获取应用程序的指令和内存吞吐量


如果将程序的 `测量值` 与 `理论峰值` 进行比较，可以判定你的程序的性能是受限于算法还是受限于内存带宽

以 Tesla K10 为例

>Tesla K10 单精度浮点运算次数
```c++
// FLOPS表示每秒浮点运算次数
TFLOPS= 745 MHz核心频率 * 2 GPU/芯片 * (8个多处理器*192个浮点单元*32核心/多处理器）*2 OPS/周期＝4.58 
```

>Tesla K10 内存带宽峰值
```c++
内存带宽峰值 = 2 GPU/芯片*256位*2500 MHz内存时钟*2 DDR/8位/字节＝320 GB/s
```
>指令比
```c++
4.58 TFLOPS/320 GB/s // 也就是13.6个指令：1个字节
```

对于 Tesla K10 而言，如果你的应用程序每访问一个字节所产生的指令数多于 13.6，那么你的应用程序受算法性能限制

大多数HPC工作负载受内存带宽的限制


&emsp;
## 3 代码
>test.cu
```c++
void initialData(float *ip, int size)
{
    // generate different seed for random number
    time_t t;
    srand((unsigned) time(&t));

    for (int i = 0; i < size; i++)
        ip[i] = (float)( rand() & 0xFF ) / 10.0f;
    return;
}

__global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) C[i] = A[i] + B[i];
}

int main(int argc, char **argv)
{
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    checkRuntime(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    checkRuntime(cudaSetDevice(dev));

    // set up data size of vectors
    // 1<<10(bytes) = 1KB; 1<<20(bytes)=1MB; 1<<4=16 
    int nElem = 1 << 24;
    printf("Vector size %d\n", nElem);

    // malloc host memory
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A     = (float *)malloc(nBytes);
    h_B     = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef  = (float *)malloc(nBytes);

    double iStart, iElaps;

    // initialize data at host side
    iStart = cpuTimer();
    initialData(h_A, nElem);
    initialData(h_B, nElem);
    iElaps = cpuTimer() - iStart;
    printf("initialData Time elapsed %f sec\n", iElaps);
    memset(hostRef, 0, nBytes);
    memset(gpuRef,  0, nBytes);

    // malloc device global memory
    float *d_A, *d_B, *d_C;
    checkRuntime(cudaMalloc(&d_A, nBytes));
    checkRuntime(cudaMalloc(&d_B, nBytes));
    checkRuntime(cudaMalloc(&d_C, nBytes));

    // transfer data from host to device
    checkRuntime(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));
    checkRuntime(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));
    checkRuntime(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));

    // invoke kernel at host side
    int iLen = 512;
    dim3 block (iLen);
    dim3 grid  ((nElem + block.x - 1) / block.x);

    iStart = cpuTimer();
    sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);
    checkRuntime(cudaDeviceSynchronize());
    iElaps = cpuTimer() - iStart;
    printf("sumArraysOnGPU <<<%d, %d>>>  Time elapsed %f sec\n", grid.x,
           block.x, iElaps);

    // check kernel error
    checkRuntime(cudaGetLastError()) ;

    // copy kernel result back to host side
    checkRuntime(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));

    // free device global memory
    checkRuntime(cudaFree(d_A));
    checkRuntime(cudaFree(d_B));
    checkRuntime(cudaFree(d_C));

    return(0);
}
```