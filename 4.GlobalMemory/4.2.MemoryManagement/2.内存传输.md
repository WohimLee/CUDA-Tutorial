
# 内存传输

## 1 API
>cudaMemcpy
```c++
cudaErrot_t cudaMemcpy(
    void* pDst, const void* pSrc, size_t nBytes,
    enum cudaMemcpyKind kind
);
```
- 这个函数从内存位置 pSrc 复制了 nBytes 字节到内存位置 pDst
- kind指定了复制的方向，可以有下列取值：
    - cudaMemcpyHostToHost
    - cudaMemcpyHostToDevice
    - cudaMemcpyDeviceToHost
    - cudaMemcpyDeviceToDevice
- 这个函数在大多数情况下都是同步的

&emsp;
## 2 CPU 与 GPU 之间的数据传输
GPU 芯片和 `板载GDDR5 GPU内存` 之间的理论峰值带宽非常高，对于Fermi C2050 GPU来说为144GB/s。 
 
CPU 和 GPU 之间通过 `PCIe Gen2总线`相连，这种连接的理论带宽要低得多，为8GB/s（PCIe Gen3总线最大理论限制值是16GB/s）。

这种差距意味着如果管理不当的话，主机和设备 间的数据传输会降低应用程序的整体性能。因此，CUDA 编程的一个基本原则应是尽可能地减少主机与设备之间的传输。
<div align=center>
    <image src="imgs/2-1.png" width=400>
</div>